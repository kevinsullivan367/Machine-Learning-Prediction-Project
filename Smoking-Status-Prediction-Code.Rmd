---
title: "BIOS635 Midterm"
author: "Kevin Sullivan"
date: '2025-02-22'
output: html_document
---

```{r, eval = FALSE}
# Libraries

library(dplyr)
library(nnet)
library(glmnet)
library(caret)
library(e1071)
library(MASS)
library(ISLR)
library(boot)
library(gam)
library(combinat)
library(class)
library(knitr)
library(kableExtra)
library(htmltools)
library(webshot)
```


```{r}
# Read in Data

train_data = read.csv("train.csv")

test_data = read.csv("test.csv")
```


**EDA and Data Pre-processing to Systematically Reduce the Number of Features**


```{r}

full_logistic_model = glm(smoking ~ ., data = train_data, family = "binomial")
summary(full_logistic_model)

train_data = train_data %>%
  mutate(BMI = round((weight.kg. / (height.cm.)^2) * 10000, 2)) %>%
  dplyr::select(BMI, everything())

test_data = test_data %>%
  mutate(BMI = round((weight.kg. / (height.cm.)^2) * 10000, 2)) %>%
  dplyr::select(BMI, everything())

train_data_key_features_literature = train_data %>%
  dplyr::select(c(age, BMI, relaxation, systolic, HDL, LDL, Cholesterol, triglyceride, dental.caries, smoking))

summary(train_data_key_features_literature)

```


```{r}
# Create Validation Sets (Full and Reduced)

# Reduced
target = "smoking"

features = c("age", "BMI", "relaxation", "systolic", "HDL", "LDL", "Cholesterol", "triglyceride", "dental.caries")

train_data_key_features_literature[[target]] = as.factor(train_data_key_features_literature[[target]])

set.seed(1829)

train_index_reduced = createDataPartition(train_data_key_features_literature[[target]], p = 0.7, list = FALSE)
train_data_reduced = train_data_key_features_literature[train_index_reduced, ]
validation_data_reduced = train_data_key_features_literature[-train_index_reduced, ]

levels_train = levels(train_data_key_features_literature[[target]])
train_data_key_features_literature[[target]] = factor(train_data_key_features_literature[[target]], levels = levels_train)

validation_data_reduced[[target]] = factor(validation_data_reduced[[target]], levels = levels_train)

X_validation_reduced = as.matrix(validation_data_reduced[, features])
y_validation_reduced = validation_data_reduced[[target]]

# Full

set.seed(1829)
train_index_full = createDataPartition(train_data[[target]], p = 0.7, list = FALSE)
train_data_full = train_data[train_index_full, ]
validation_data_full = train_data[-train_index_full, ]

X_validation_full = as.matrix(validation_data_full)
y_validation_full= validation_data_full[[target]]
y_validation_full = as.factor(validation_data_full[[target]])
```


**Logistic Regression**


```{r}
# Iterate Through All Possible Combinations of Features

feature_combinations = list()

for (i in 1:length(features)) {
  feature_combinations = c(feature_combinations, combn(features, i, simplify = FALSE))
}

results = data.frame(model = character(), misclassification_rate = numeric(), stringsAsFactors = FALSE)

for (comb in feature_combinations) {
  
  formula = as.formula(paste(target, "~", paste(comb, collapse = " + ")))
  
  model = glm(formula, data = train_data_key_features_literature, family = binomial())
  
  predictions = predict(model, type = "response")
  predicted_classes = ifelse(predictions > 0.5, 1, 0)
  
  confusion = confusionMatrix(as.factor(predicted_classes), train_data_key_features_literature[[target]])
  misclassification_rate = 1 - confusion$overall['Accuracy']
  
  results = rbind(results, data.frame(model = paste(comb, collapse = ", "), misclassification_rate = misclassification_rate))
}


```


```{r}
# Find Best Model (lowest misclassification error)

best_logistic_model = results[which.min(results$misclassification_rate), ]
print(best_logistic_model)

best_logistic_model_fit = glm(smoking ~ age + relaxation + systolic + HDL + Cholesterol + triglyceride + dental.caries, data = train_data, family = "binomial")

summary(best_logistic_model_fit)
```


**KNN**


```{r}
# Data Preprocessing 

# Split Training Data into Training and Validation Sets 70/30 
test_index = sample(nrow(train_data_key_features_literature), 0.3*nrow(train_data_key_features_literature), replace=F)
knn_train = train_data_key_features_literature[-test_index,]
knn_test = train_data_key_features_literature[test_index,]

knn_train$smoking = factor(knn_train$smoking, levels = c(0,1))
knn_test$smoking = factor(knn_test$smoking, levels = c(0,1))
```


```{r}
# K = 1-100 for reduced training set

all_predictors = setdiff(names(knn_train), "smoking")

results_list = list()

for (subset_size in 3:9) {
  predictor_combinations = combn(all_predictors, subset_size, simplify = FALSE)
  
  for (predictor_set in predictor_combinations) {
    
    formula_str = paste("smoking ~", paste(predictor_set, collapse = " + "))
    formula_obj = as.formula(formula_str)
    
    accuracy_results = numeric(100)
    
    for (k in 1:100) {
      knn_fit = knn3(formula_obj, data = knn_train, k = k)
      
      y_hat_knn = predict(knn_fit, knn_test, type = "class")
      
      accuracy = confusionMatrix(y_hat_knn, knn_test$smoking)$overall["Accuracy"]
      
      accuracy_results[k] = accuracy
    }
    
    results_list[[paste(predictor_set, collapse = "_")]] = accuracy_results
  }
}

results_list

```


```{r}
# Find Best Model (lowest misclassification error)

results_df = data.frame(
  Model = names(results_list),
  Max_Accuracy = sapply(results_list, max),  
  Best_k = sapply(results_list, function(acc) which.max(acc))  
)

top_models = results_df %>%
  arrange(desc(Max_Accuracy)) %>%
  head(3)  

print(top_models)

```


*LDA, QDA, Naive Bayes**


```{r}
# LDA (Iterate through all possible combinations of features)

feature_combinations_lda = list()

for (i in 1:length(features)) {
  feature_combinations_lda = c(feature_combinations_lda, combn(features, i, simplify = FALSE))
}

results_lda = data.frame(model = character(), misclassification_rate_lda = numeric(), stringsAsFactors = FALSE)

for (comb_lda in feature_combinations_lda) {
  
  formula_lda = as.formula(paste(target, "~", paste(comb_lda, collapse = " + ")))
  
  model_lda = lda(formula_lda, data = train_data_key_features_literature)
  
  predictions_lda = predict(model_lda, type = "response")
  predicted_classes_lda = ifelse(predictions_lda$posterior[,2] > 0.5, 1, 0)
  
  confusion_lda = confusionMatrix(as.factor(predicted_classes_lda), train_data_key_features_literature[[target]])
  misclassification_rate_lda = 1 - confusion_lda$overall['Accuracy']
  
  results_lda = rbind(results_lda, data.frame(model_lda = paste(comb_lda, collapse = ", "), misclassification_rate_lda = misclassification_rate_lda))
}

```

```{r}
# Best LDA Model (lowest misclassification error)

best_lda_model = results_lda[which.min(results_lda$misclassification_rate_lda), ]
print(best_lda_model)

best_lda_model_fit = lda(smoking ~ age + relaxation + systolic + HDL + LDL + triglyceride + dental.caries, data = train_data_key_features_literature)

summary(best_lda_model_fit)


```


```{r}
# QDA (Iterate through all possible combinations of features)

feature_combinations_qda = list()

for (i in 1:length(features)) {
  feature_combinations_qda = c(feature_combinations_qda, combn(features, i, simplify = FALSE))
}

results_qda = data.frame(model = character(), misclassification_rate_qda = numeric(), stringsAsFactors = FALSE)

for (comb_qda in feature_combinations_qda) {
  
  formula_qda = as.formula(paste(target, "~", paste(comb_qda, collapse = " + ")))
  
  model_qda = qda(formula_qda, data = train_data_key_features_literature)
  
  predictions_qda = predict(model_qda, type = "response")
  predicted_classes_qda = ifelse(predictions_qda$posterior[,2] > 0.5, 1, 0)
  
  confusion_qda = confusionMatrix(as.factor(predicted_classes_qda), train_data_key_features_literature[[target]])
  misclassification_rate_qda = 1 - confusion_qda$overall['Accuracy']
  
  results_qda = rbind(results_qda, data.frame(model_qda = paste(comb_qda, collapse = ", "), misclassification_rate_qda = misclassification_rate_qda))
}

```


```{r}
# Best QDA Model (lowest misclassification error)

best_qda_model = results_qda[which.min(results_qda$misclassification_rate_qda), ]
print(best_qda_model)


best_qda_model_fit = qda(smoking ~ age + LDL + Cholesterol + triglyceride , data = train_data_key_features_literature)

summary(best_qda_model_fit)

```




```{r}
# Naive Bayes (Iterate through all possible combinations of variables)

features = colnames(train_data_key_features_literature)[!colnames(train_data_key_features_literature) %in% c("smoking")]  

feature_combinations_nb = list()

for (i in 1:length(features)) {
  feature_combinations_nb = c(feature_combinations_nb, combn(features, i, simplify = FALSE))
}

results_nb = data.frame(model = character(), misclassification_rate_nb = numeric(), stringsAsFactors = FALSE)

for (comb_nb in feature_combinations_nb) {
  
  formula_nb = as.formula(paste("smoking ~", paste(comb_nb, collapse = " + ")))
  
  model_nb = naiveBayes(formula_nb, data = train_data_key_features_literature)
  
  nb_posterior = predict(model_nb, validation_data_reduced, type = 'raw')
  
  nb_prediction = ifelse(nb_posterior[, 2] > 0.5, 1, 0) 
  
  validation_data_reduced$smoking = factor(validation_data_reduced$smoking, levels = c(0, 1))
  
  confusion_nb = confusionMatrix(factor(nb_prediction, levels = c(0, 1)), validation_data_reduced$smoking)
  misclassification_rate_nb = 1 - confusion_nb$overall['Accuracy']
  
  results_nb = rbind(results_nb, data.frame(model = paste(comb_nb, collapse = ", "), misclassification_rate_nb = misclassification_rate_nb))
}

```


```{r}
# Best Naive Bayes Model (Lowest misclassification error)

best_nb_model = results_nb[which.min(results_nb$misclassification_rate_nb), ]
print(best_nb_model)

best_nb_model_fit = naiveBayes(smoking ~ age + relaxation + systolic + HDL + Cholesterol + triglyceride , data = train_data_key_features_literature)

```


**5 and 10 Fold Cross Validation**


```{r}
# 5 Fold CV

cv_error_5 = rep(NA, length(features))

var_combinations_5CV = unlist(lapply(1:length(features), function(x) combn(features, x, simplify = FALSE)), recursive = FALSE)

for (i in 1:length(var_combinations_5CV)) {
  current_vars = var_combinations_5CV[[i]]
  
  formula_5CV = as.formula(paste("smoking ~", paste(current_vars, collapse = "+")))
  
  # Fit the logistic regression model
  glm_fit1 = glm(formula_5CV, data = train_data_key_features_literature, family = 'binomial')
  
  # Calculate cross-validation error using 5-fold cross-validation
  cv_error_5[i] = cv.glm(train_data_key_features_literature, glm_fit1, K=5)$delta[1]
}


# Find Best 5-Fold CV 

best_model_5CV = which.min(cv_error_5)

best_combination_5CV = var_combinations_5CV[[best_model_5CV]]

best_error_5CV = cv_error_5[best_model_5CV]

best_error_5CV

best_combination_5CV


```


```{r}
# Fit Best 5 Fold CV 

best_formula_5CV = as.formula(paste("smoking ~", paste(best_combination_5CV, collapse = "+")))

final_5CV_model = glm(best_formula_5CV, data = train_data_key_features_literature, family = 'binomial')

summary(final_5CV_model)
```



```{r}
# 10 Fold CV

cv_error_10 = rep(NA, length(features))

var_combinations_10CV = unlist(lapply(1:length(features), function(x) combn(features, x, simplify = FALSE)), recursive = FALSE)

for (i in 1:length(var_combinations_10CV)) {
  current_vars_10CV = var_combinations_10CV[[i]]
  
  formula_10CV = as.formula(paste("smoking ~", paste(current_vars_10CV, collapse = "+")))
  
  glm_fit2 = glm(formula_10CV, data = train_data_key_features_literature, family = 'binomial')
  
  cv_error_10[i] = cv.glm(train_data_key_features_literature, glm_fit2, K=10)$delta[1]
}

# Find Best 10-Fold CV 

best_model_10CV = which.min(cv_error_10)

best_combination_10CV = var_combinations_10CV[[best_model_10CV]]

best_error_10CV = cv_error_10[best_model_10CV]

best_combination_10CV

best_error_10CV

cv_error_10

cv_error_5

```


```{r}
# Fit Best 10-Fold CV

best_formula_10CV = as.formula(paste("smoking ~", paste(best_combination_10CV, collapse = "+")))

final_10CV_model = glm(best_formula_10CV, data = train_data_key_features_literature, family = 'binomial')

summary(final_10CV_model)
```



**# Forward, Backward, Forward-Backward Stepwise Selection**

```{r, eval = FALSE}
# Forward Selection

null_logit_model = glm(smoking ~ 1, data = train_data, family = "binomial")

full_logit_model = glm(smoking ~ ., data = train_data, family = "binomial")

forward_selection_model = stepAIC(null_logit_model, 
                         scope = list(lower = null_logit_model, upper = full_logit_model), 
                         direction = "forward", 
                         trace = 0)  

```


```{r}
# Preferred Forward Selection Model

preferred_forward_select = glm(smoking ~ hemoglobin + height.cm. + Gtp + triglyceride + 
    Cholesterol + ALT + dental.caries + weight.kg. + BMI + systolic + 
    fasting.blood.sugar + AST + HDL + relaxation + Urine.protein + 
    eyesight.right. + waist.cm. + LDL + hearing.right., data = train_data, family = "binomial")

summary(preferred_forward_select)

```


```{r, eval = FALSE}
# Backward Selection

backward_selection_model = stepAIC(full_logit_model, 
                         scope = list(lower = null_logit_model, upper = full_logit_model), 
                         direction = "backward", 
                         trace = 0)  

```


```{r}
# Preferred Backward Selection Model

preferred_backward_select = glm(smoking ~ BMI + height.cm. + weight.kg. + waist.cm. + eyesight.right. + 
    hearing.right. + systolic + relaxation + fasting.blood.sugar + 
    Cholesterol + triglyceride + HDL + LDL + hemoglobin + Urine.protein + 
    AST + ALT + Gtp + dental.caries, data = train_data, family = "binomial")

summary(preferred_backward_select)
```


```{r}
# Forward-Backward Stepwise Selection

preferred_both_select = stepAIC(null_logit_model, 
                         scope = list(lower = null_logit_model, upper = full_logit_model), 
                         direction = "both", 
                         trace = 0) 
```


```{r}
# Forward-Backward Selection Yields the same optimal model as Forward Selection
```

```{r}
# Calculate Misclassification Error for Each

# Forward
predictions_forward = predict(preferred_forward_select, newdata = X_validation_df, type = "response")

predicted_classes_forward = ifelse(predictions_forward > 0.5, 1, 0)

predicted_classes_forward = factor(predicted_classes_forward, levels = c(0, 1))
y_validation_full = factor(y_validation_full, levels = c(0, 1))

confusion_forward = confusionMatrix(predicted_classes_forward, y_validation_full)
misclassification_rate_forward = 1 - confusion_forward$overall['Accuracy']

misclassification_rate_forward


# Backward

predictions_backward = predict(preferred_backward_select, newdata = X_validation_df, type = "response")

predicted_classes_backward = ifelse(predictions_backward > 0.5, 1, 0)

predicted_classes_backward = factor(predicted_classes_backward, levels = c(0, 1))
y_validation_full = factor(y_validation_full, levels = c(0, 1))

confusion_backward = confusionMatrix(predicted_classes_backward, y_validation_full)
misclassification_rate_backward = 1 - confusion_backward$overall['Accuracy']

misclassification_rate_backward
```


**Ridge and Lasso Regression**

```{r}
# Preprocessing for Lasso and Ridge

x = model.matrix(smoking ~ ., train_data)[, -1]
y =  train_data$smoking
dim(x)

```


```{r}
# Ridge Regression

grid = 10^seq(10, -4, length = 100)

ridge_model = glmnet(x, y, family = "binomial", alpha = 0, lambda = grid)

min(abs(coef(ridge_model)))

coef(ridge_model)[1:5,1:5]

```


```{r}
plot(log(ridge_model$lambda), coef(ridge_model)[2,] / sd(coef(ridge_model)[2,]), 
     type = 'l', col = 2, ylim = c(-4, 4))
for(i in 3:nrow(coef(ridge_model))) {
  lines(log(ridge_model$lambda), coef(ridge_model)[i,] / sd(coef(ridge_model)[i,]), 
        col = i)
}
```

```{r}
set.seed(1829)
cv_out = cv.glmnet(x, y, alpha=0)
plot(cv_out)

bestlam = cv_out$lambda.min
bestlam
```


```{r}
# Ridge Predictions

ridge_pred = predict(ridge_model, as.matrix(validation_data_full[, -5]), s = bestlam, type = "response")

ridge_pred_class = ifelse(ridge_pred > 0.5, 1, 0)

conf_matrix_ridge = confusionMatrix(factor(ridge_pred_class), factor(y_validation_full))

misclassification_error_ridge = 1 - conf_matrix_ridge$overall['Accuracy']

misclassification_error_ridge

```

```{r}
# Lasso Regression

lasso_model = glmnet(x, y, family = "binomial", alpha = 1)

plot(lasso_model, xvar = "dev", label = TRUE)

```

```{r}
cvfit = cv.glmnet(x, y, family = "binomial", type.measure = "class")

plot(cvfit)

bestlam_lasso = cvfit$lambda.min
bestlam_lasso

```


```{r}
# Lasso Predictions

lasso_pred = predict(lasso_model, as.matrix(validation_data_full[, -5]), s = bestlam_lasso, type = "response")

lasso_pred_class = ifelse(lasso_pred > 0.5, 1, 0)

conf_matrix_lasso = confusionMatrix(factor(lasso_pred_class), factor(y_validation_full))

misclassification_error_lasso = 1 - conf_matrix_lasso$overall['Accuracy']

misclassification_error_lasso
```


**Preferred Models Predictions on Test Data and Export as CSV to Submit to Kaggle**


```{r}
# Logistic
predicted_probs_best_logistic = predict(best_logistic_model_fit, newdata = test_data, type = "response")

best_logistic_kaggle = data.frame(id = test_data$id, smoking = predicted_probs_best_logistic)

write.csv(best_logistic_kaggle, file = "logistic_kaggle.csv", row.names = FALSE)

```

```{r}
# KNN

# Model 1
knn_fit1= knn3(smoking ~ age + BMI + relaxation, data = knn_train, k = 1)

predicted_probs = predict(knn_fit1, test_data, type = "prob")

results_df1 = data.frame(id = test_data$id, smoking = predicted_probs[, 2])

write.csv(results_df1, "knn_predictions1.csv", row.names = FALSE)

# Model 2

knn_fit2 = knn3(smoking ~ age + BMI + HDL, data = knn_train, k = 1)

predicted_probs2 = predict(knn_fit2, test_data, type = "prob")

results_df2 = data.frame(id = test_data$id, smoking = predicted_probs2[, 2])

write.csv(results_df2, "knn_predictions2.csv", row.names = FALSE)
```



```{r}
# LDA

predicted_probs_best_lda = predict(best_lda_model_fit, newdata = test_data, type = "response")

best_lda_kaggle = data.frame(id = test_data$id, smoking = predicted_probs_best_lda)

write.csv(best_lda_kaggle, file = "lda_kaggle.csv", row.names = FALSE)
```

```{r}
# QDA

predicted_probs_best_qda = predict(best_qda_model_fit, newdata = test_data, type = "response")

best_qda_kaggle = data.frame(id = test_data$id, smoking = predicted_probs_best_qda)

write.csv(best_qda_kaggle, file = "qda_kaggle.csv", row.names = FALSE)

```


```{r}
# Naive Bayes

predicted_probs_best_nb = predict(best_nb_model_fit, newdata = test_data, type = "raw")

best_nb_kaggle = data.frame(id = test_data$id, smoking = predicted_probs_best_nb)

write.csv(best_nb_kaggle, file = "nb_kaggle.csv", row.names = FALSE)
```


```{r}
# 5CV

predicted_probs_best_5CV = predict(final_5CV_model, newdata = test_data, type = "response")

best_5CV_kaggle = data.frame(id = test_data$id, smoking = predicted_probs_best_5CV)

write.csv(best_5CV_kaggle, file = "5CV_kaggle.csv", row.names = FALSE)

```


```{r}
# 10CV

predicted_probs_best_10CV = predict(final_10CV_model, newdata = test_data, type = "response")

best_10CV_kaggle = data.frame(id = test_data$id, smoking = predicted_probs_best_10CV)

write.csv(best_10CV_kaggle, file = "10CV_kaggle.csv", row.names = FALSE)
```


```{r}
# Forward Select

predicted_probs_best_forward = predict(preferred_forward_select, newdata = test_data, type = "response")

best_forward_kaggle = data.frame(id = test_data$id, smoking = predicted_probs_best_forward)

write.csv(best_forward_kaggle, file = "forward_kaggle.csv", row.names = FALSE)

```


```{r}
# Backward Select

predicted_probs_best_backward = predict(preferred_backward_select, newdata = test_data, type = "response")

best_backward_kaggle = data.frame(id = test_data$id, smoking = predicted_probs_best_backward)

write.csv(best_backward_kaggle, file = "backward_kaggle.csv", row.names = FALSE)

```

```{r}
# Lasso

predicted_probs_best_lasso = predict(lasso_model, newdata = test_data, type = "response")

best_lasso_kaggle = data.frame(id = test_data$id, smoking = predicted_probs_best_lasso)

write.csv(best_lasso_kaggle, file = "lasso_kaggle.csv", row.names = FALSE)
```



**Build Tables for Report**


```{r}
# Table 1

table_data = data.frame(
  Algorithm = c("Logistic", "kNN", "kNN", "LDA", "QDA", "Naive Bayes", "5-Fold Cross-Validation†", 
                "10-Fold Cross-Validation", "Forward Select", "Lasso"),
  Model = c(
    "*Smoking* ~ Age + Relaxation + Systolic + HDL + Cholesterol + Triglyceride + Dental Caries",
    "*Smoking* ~ Age + BMI + Relaxation",
    "*Smoking* ~ Age + BMI + HDL",
    "*Smoking* ~ Age + Relaxation + Systolic + HDL + LDL + Triglyceride + Dental Caries",
    "*Smoking* ~ Age + LDL + Cholesterol + Triglyceride",
    "*Smoking* ~ Age + Relaxation + Systolic + HDL + Cholesterol + Triglyceride",
    "*Smoking* ~ Age + HDL + LDL + Triglyceride + Dental Caries",
    "*Smoking* ~ Age + Cholesterol + Triglyceride + Dental Caries",
    "*Smoking* ~ Hemoglobin + Height (cm) + Gtp + Triglyceride + 
    Cholesterol + ALT + Dental Caries + Weight (kg) + BMI + Systolic + 
    Fasting Blood Sugar + AST + HDL + Relaxation + Urine Protein + 
    Eyesight (Right Eye) + Waist Size (cm) + LDL + Hearing (Right Ear)",
    "*Smoking* ~ Age + BMI + Cholesterol + Dental Caries + LDL + HDL + Triglyceride + Systolic + Relaxation"
  ),
  Remarks = c("", "k = 1", "k = 1", "", "", "", "All models with 5-fold CV performed better than all other methods. No 5-fold CV model had an error rate > 25%", "", "Stepwise", "λ = 0.000438"),
  Error = c(0.319, 0.314, 0.316, 0.319, 0.323, 0.322, 0.204, 0.204, 0.2508, 0.385),
  ROC_score = c(0.754, 0.735, 0.723, 0.755, 0.737, 0.738, 0.753, 0.7507, 0.838, 0.620)
)

pub_table = kable(table_data, format = "html", escape = FALSE, 
                    col.names = c("Algorithm", "Model", "Remarks", "Error*", "Kaggle Score**")) %>%
  kable_styling("striped", full_width = FALSE) %>%
  row_spec(7, background = "#d9ead3") %>%
  column_spec(2, width = "30em") %>%
  add_footnote("*Misclassification error* \n **ROC score (Higher score = better) \n †Preferred final model")
 
pub_table


```




